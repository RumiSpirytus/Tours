{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8qeG3y41L2O",
        "outputId": "306abc79-462c-4f45-88c9-46101263953a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/nguyen-duc-thien/Tours/AI\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nguyen-duc-thien/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "HOME = '/home/nguyen-duc-thien/Tours/AI'\n",
        "%cd {HOME}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-12-09T19:07:20.506465Z",
          "iopub.status.busy": "2024-12-09T19:07:20.505979Z"
        },
        "id": "QpgjKrOiFCNt",
        "outputId": "79682c25-9097-4caa-e8e0-cb2f47867d84",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/nguyen-duc-\n",
            "[nltk_data]     thien/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/nguyen-duc-\n",
            "[nltk_data]     thien/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /home/nguyen-duc-\n",
            "[nltk_data]     thien/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import pickle\n",
        "from collections import Counter, defaultdict\n",
        "from scipy.spatial.distance import cosine\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mywJTakomEQG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_1iT_9w16Ib",
        "outputId": "8883c504-894e-4f72-f18b-15f54237e81e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              Review  Rating     Label\n",
            "0  nice hotel expensive parking got good deal sta...       4  positive\n",
            "1  ok nothing special charge diamond member hilto...       2  negative\n",
            "2  nice rooms not 4* experience hotel monaco seat...       3  positive\n",
            "3  unique, great stay, wonderful time hotel monac...       5  positive\n",
            "4  great stay great stay, went seahawk game aweso...       5  positive\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_csv(\"/home/nguyen-duc-thien/Tours/AI/output.csv\")\n",
        "dataset = dataset[:500]\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFvPvaLgFZa1",
        "outputId": "f933374d-99a8-4955-95ab-864dfdba7e3d"
      },
      "source": [
        "dataset = pd.read_csv(\"/kaggle/input/hotels/output.csv\")\n",
        "dataset = dataset[:100]\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "cA-dP9pjGZPu",
        "outputId": "b86484a4-8b42-4f69-d077-c35f1371dd66",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaN values in the dataset:\n",
            "Review    0\n",
            "Rating    0\n",
            "Label     0\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'ok nothing special charge diamond member hilton decided chain shot 20th anniversary seattle start booked suite paid extra website description suite bedroom bathroom standard hotel room took printed reservation desk showed said thing like tv couch ect desk clerk told oh mixed suite description kimpton website sorry free breakfast got kidding embassy suit sitting room bathroom bedroom unlike kimpton call suite 5 day stay offer correct false advertising send kimpton preferred guest website email asking failure provide suite advertised website reservation description furnished hard copy reservation printout website desk manager duty reply solution send email trip guest survey follow email mail guess tell concerned guest staff ranged indifferent helpful asked desk good breakfast spot neighborhood hood told hotel gee best breakfast spot seattle 1 2 block away convenient hotel know exist arrived late night 11 pm inside run bellman busy chating cell phone help bag prior arrival emailed hotel inform 20th anniversary half really picky wanted make sure good got nice email saying like deliver bottle champagne chocolate covered strawberry room arrival celebrate told needed foam pillow arrival champagne strawberry foam pillow great room view alley high rise building good better housekeeping staff cleaner room property impressed left morning shopping room got short trip 2 hour bed comfortable good ac heat control 4 x 4 inch screen bring green shine directly eye night light sensitive tape control 4 start hotel clean business hotel super high rate better chain hotel seattle'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"NaN values in the dataset:\")\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "# Remove rows with NaN values or fill them\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "def remove_tags(text):\n",
        "  remove = re.compile(r'')\n",
        "  return re.sub(remove, '', text)\n",
        "dataset['Review'] = dataset['Review'].apply(remove_tags)\n",
        "\n",
        "def special_char(text):\n",
        "  reviews = ''\n",
        "  for x in text:\n",
        "    if x.isalnum():\n",
        "      reviews = reviews + x\n",
        "    else:\n",
        "      reviews = reviews + ' '\n",
        "  return reviews\n",
        "dataset['Review'] = dataset['Review'].apply(special_char)\n",
        "\n",
        "def convert_lower(text):\n",
        "   return text.lower()\n",
        "dataset['Review'] = dataset['Review'].apply(convert_lower)\n",
        "dataset['Review'][1]\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = word_tokenize(text)\n",
        "  return [x for x in words if x not in stop_words]\n",
        "dataset['Review'] = dataset['Review'].apply(remove_stopwords)\n",
        "dataset['Review'][1]\n",
        "\n",
        "def lemmatize_word(text):\n",
        "  wordnet = WordNetLemmatizer()\n",
        "  return \" \".join([wordnet.lemmatize(word) for word in text])\n",
        "dataset['Review'] = dataset['Review'].apply(lemmatize_word)\n",
        "dataset['Review'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl5_GC_exwek",
        "outputId": "c14b7b7f-a977-46a8-d690-96ca75da6aa9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500\n",
            "0    nice hotel expensive parking got good deal sta...\n",
            "1    ok nothing special charge diamond member hilto...\n",
            "2    nice room 4 experience hotel monaco seattle go...\n",
            "3    unique great stay wonderful time hotel monaco ...\n",
            "4    great stay great stay went seahawk game awesom...\n",
            "Name: Review, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Training data and labels\n",
        "training_texts = dataset['Review']\n",
        "training_labels = dataset['Label']\n",
        "\n",
        "print(len(dataset))\n",
        "print(dataset.head()['Review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OGREhXK548kX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def save_model(classifier, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(classifier, file)\n",
        "\n",
        "def load_model(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        return pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L302wg3dgYHN"
      },
      "source": [
        "## Text Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EZ-rno7gxQrN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TextClassifier:\n",
        "    def __init__(self):\n",
        "        self.training_data = []\n",
        "        self.vocabulary = set()\n",
        "        self.idf_weights = {}\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return text.lower().split()\n",
        "\n",
        "    def _build_vocabulary(self, texts):\n",
        "        self.vocabulary = set()\n",
        "        for text in texts:\n",
        "            self.vocabulary.update(self._tokenize(text))\n",
        "\n",
        "    def _compute_tfidf_weights(self, texts):\n",
        "        # Count document frequencies\n",
        "        doc_freq = defaultdict(int)\n",
        "        total_docs = len(texts)\n",
        "\n",
        "        for text in texts:\n",
        "            unique_terms = set(self._tokenize(text))\n",
        "            for term in unique_terms:\n",
        "                doc_freq[term] += 1\n",
        "\n",
        "        # Compute IDF weights\n",
        "        self.idf_weights = {\n",
        "            term: math.log(total_docs / (count + 1))\n",
        "            for term, count in doc_freq.items()\n",
        "        }\n",
        "\n",
        "    def _text_to_vector(self, text):\n",
        "        tokens = self._tokenize(text)\n",
        "        vector = [0] * len(self.vocabulary)\n",
        "\n",
        "        # Count term frequencies\n",
        "        token_counts = Counter(tokens)\n",
        "\n",
        "        for i, term in enumerate(self.vocabulary):\n",
        "            # TF-IDF calculation\n",
        "            tf = token_counts.get(term, 0) / len(tokens)\n",
        "            idf = self.idf_weights.get(term, 0)\n",
        "            vector[i] = tf * idf\n",
        "\n",
        "        return vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfuCNdRYgdwb"
      },
      "source": [
        "## KNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KJC4oNG4CxGC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class KNNClassifier(TextClassifier):\n",
        "    def __init__(self, k=5):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.training_vectors = None\n",
        "        self.training_labels = None\n",
        "\n",
        "    def _cosine_similarity(self, vec1, vec2):\n",
        "        dot_product = sum(x*y for x, y in zip(vec1, vec2))\n",
        "        magnitude1 = math.sqrt(sum(x*x for x in vec1))\n",
        "        magnitude2 = math.sqrt(sum(x*x for x in vec2))\n",
        "\n",
        "        return dot_product / (magnitude1 * magnitude2 + 1e-10)\n",
        "\n",
        "    # def train(self, training_texts, labels):\n",
        "    #     # Build vocabulary and compute IDF weights\n",
        "    #     self._build_vocabulary(training_texts)\n",
        "    #     self._compute_tfidf_weights(training_texts)\n",
        "\n",
        "    #     # Compute vector representations\n",
        "    #     self.training_data = [\n",
        "    #         (self._text_to_vector(text), label)\n",
        "    #         for text, label in zip(training_texts, labels)\n",
        "    #     ]\n",
        "\n",
        "    def train(self, training_texts, labels):\n",
        "        # Build vocabulary and compute IDF weights\n",
        "        self._build_vocabulary(training_texts)\n",
        "        self._compute_tfidf_weights(training_texts)\n",
        "\n",
        "        # Convert to numpy arrays for faster computations\n",
        "        self.training_vectors = np.array([\n",
        "            self._text_to_vector(text) for text in training_texts\n",
        "        ])\n",
        "        self.training_labels = np.array(labels)\n",
        "\n",
        "    # def predict(self, new_text):\n",
        "    #     new_vector = self._text_to_vector(new_text)\n",
        "    #     point_non_zero = [point for point in new_vector if point != 0.0]\n",
        "    #     print(point_non_zero)\n",
        "\n",
        "    #     # Compute similarities to training vectors\n",
        "    #     similarities = [\n",
        "    #         (self._cosine_similarity(new_vector, train_vector), label)\n",
        "    #         for train_vector, label in self.training_data\n",
        "    #     ]\n",
        "\n",
        "    #     # Sort and get k nearest neighbors\n",
        "    #     similarities.sort(reverse=True, key=lambda x: x[0])\n",
        "    #     print(similarities)\n",
        "    #     k_nearest = similarities[:self.k]\n",
        "\n",
        "    #     # Majority voting\n",
        "    #     label_counts = Counter(label for _, label in k_nearest)\n",
        "    #     return label_counts.most_common(1)[0][0]\n",
        "\n",
        "    def predict(self, new_text):\n",
        "        # Convert new text to vector using numpy\n",
        "        new_vector = np.array(self._text_to_vector(new_text))\n",
        "\n",
        "        # Create a dictionary to group vectors by label\n",
        "        label_to_vectors = defaultdict(list)\n",
        "        for vector, label in zip(self.training_vectors, self.training_labels):\n",
        "            label_to_vectors[label].append(vector)\n",
        "\n",
        "        # Random 50 vectors for each class/label\n",
        "        random_train_vectors = []\n",
        "        for label, vectors in label_to_vectors.items():\n",
        "            selected_vectors = []\n",
        "            vectors = np.array(vectors)\n",
        "            if len(vectors) > 50:\n",
        "                selected_indices = np.random.choice(len(vectors), size=50, replace=False)\n",
        "                selected_vectors = vectors[selected_indices]\n",
        "            else:\n",
        "                selected_vectors = vectors\n",
        "            random_train_vectors.extend(selected_vectors)\n",
        "            print(label, len(selected_vectors))\n",
        "        # Compute similarities using vectorized operations\n",
        "        similarities = np.array([\n",
        "            self._cosine_similarity(new_vector, train_vector)\n",
        "            for train_vector in np.array(random_train_vectors)\n",
        "        ])\n",
        "\n",
        "        # Get indices of k nearest neighbors\n",
        "        k_nearest_indices = np.argpartition(similarities, -self.k)[-self.k:]\n",
        "\n",
        "        # Select labels for k nearest neighbors\n",
        "        k_nearest_labels = self.training_labels[k_nearest_indices]\n",
        "\n",
        "        # Majority voting\n",
        "        label_counts = Counter(k_nearest_labels)\n",
        "        return label_counts.most_common(1)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "or1ciGUY8MuJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create and train classifier\n",
        "knn_classifier = KNNClassifier(k=9)\n",
        "knn_classifier.train(training_texts, training_labels)\n",
        "\n",
        "# Save model\n",
        "save_model(knn_classifier, 'knn_classifier_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5cCu833giUF"
      },
      "source": [
        "## Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "S_GTiPRixUrO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier(TextClassifier):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        # Compute class probabilities\n",
        "        class_counts = Counter(labels)\n",
        "        self.class_probs = {\n",
        "            cls: count/len(labels) for cls, count in class_counts.items()\n",
        "        }\n",
        "\n",
        "        # Compute IDF weights\n",
        "        self._compute_tfidf_weights(texts)\n",
        "\n",
        "        # Term frequencies per class\n",
        "        self.term_freq = {}\n",
        "        for cls in set(labels):\n",
        "            self.term_freq[cls] = defaultdict(int)\n",
        "\n",
        "        self.class_total_terms = defaultdict(int)\n",
        "\n",
        "        # Process each document\n",
        "        for text, cls in zip(texts, labels):\n",
        "            terms = self._tokenize(text)\n",
        "            for term in terms:\n",
        "                self.term_freq[cls][term] += 1\n",
        "                self.class_total_terms[cls] += 1\n",
        "\n",
        "    def predict(self, text):\n",
        "        terms = self._tokenize(text)\n",
        "        class_scores = {}\n",
        "\n",
        "        # Compute score for each class\n",
        "        for cls in self.class_probs:\n",
        "            # Start with log of class probability\n",
        "            score = math.log(self.class_probs[cls])\n",
        "\n",
        "            # Add log probabilities of terms\n",
        "            for term in terms:\n",
        "                # Laplace smoothing\n",
        "                term_count = self.term_freq[cls][term]\n",
        "                total_terms = self.class_total_terms[cls]\n",
        "\n",
        "                # Weight term by IDF\n",
        "                idf_weight = self.idf_weights.get(term, 0)\n",
        "\n",
        "                # Compute log probability with smoothing\n",
        "                term_prob = (term_count + 1) / (total_terms + len(self.idf_weights))\n",
        "                score += math.log(term_prob) * idf_weight\n",
        "\n",
        "            class_scores[cls] = score\n",
        "\n",
        "        # Return class with highest score\n",
        "        print(class_scores)\n",
        "        return max(class_scores, key=class_scores.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DZQjU5LSfX_7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create and train classifier\n",
        "naive_bayes_classifier = NaiveBayesClassifier()\n",
        "naive_bayes_classifier.train(training_texts, training_labels)\n",
        "\n",
        "# Save model\n",
        "save_model(naive_bayes_classifier, 'bayes_classifier_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6_GfAKgnUI"
      },
      "source": [
        "## Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNiIKGMQ1RRq",
        "outputId": "602de9fa-1229-446c-872a-8ad0919bb4e3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.64      0.69      0.67        13\n",
            "    positive       0.89      0.86      0.88        37\n",
            "\n",
            "    accuracy                           0.82        50\n",
            "   macro avg       0.77      0.78      0.77        50\n",
            "weighted avg       0.82      0.82      0.82        50\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(training_texts)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, training_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = decision_tree_classifier.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PbKZb6sifdPG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save vectorizer\n",
        "save_model(vectorizer, 'vectorizer.pkl')\n",
        "\n",
        "# Save model\n",
        "save_model(decision_tree_classifier, 'tree_classifier_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fXmEoF8G1jE",
        "outputId": "5b9ca028-11cb-4542-b913-ed6f6a9cf0dd",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|--- great <= 0.02\n",
            "|   |--- location <= 0.04\n",
            "|   |   |--- comfortable <= 0.03\n",
            "|   |   |   |--- pike <= 0.02\n",
            "|   |   |   |   |--- large <= 0.07\n",
            "|   |   |   |   |   |--- wonderful <= 0.01\n",
            "|   |   |   |   |   |   |--- walking <= 0.09\n",
            "|   |   |   |   |   |   |   |--- parking <= 0.10\n",
            "|   |   |   |   |   |   |   |   |--- excellent <= 0.05\n",
            "|   |   |   |   |   |   |   |   |   |--- posh <= 0.11\n",
            "|   |   |   |   |   |   |   |   |   |   |--- kind <= 0.06\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
            "|   |   |   |   |   |   |   |   |   |   |--- kind >  0.06\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |   |--- posh >  0.11\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: positive\n",
            "|   |   |   |   |   |   |   |   |--- excellent >  0.05\n",
            "|   |   |   |   |   |   |   |   |   |--- class: positive\n",
            "|   |   |   |   |   |   |   |--- parking >  0.10\n",
            "|   |   |   |   |   |   |   |   |--- class: positive\n",
            "|   |   |   |   |   |   |--- walking >  0.09\n",
            "|   |   |   |   |   |   |   |--- class: positive\n",
            "|   |   |   |   |   |--- wonderful >  0.01\n",
            "|   |   |   |   |   |   |--- mattress <= 0.10\n",
            "|   |   |   |   |   |   |   |--- class: positive\n",
            "|   |   |   |   |   |   |--- mattress >  0.10\n",
            "|   |   |   |   |   |   |   |--- class: negative\n",
            "|   |   |   |   |--- large >  0.07\n",
            "|   |   |   |   |   |--- class: positive\n",
            "|   |   |   |--- pike >  0.02\n",
            "|   |   |   |   |--- class: positive\n",
            "|   |   |--- comfortable >  0.03\n",
            "|   |   |   |--- bag <= 0.04\n",
            "|   |   |   |   |--- carry <= 0.14\n",
            "|   |   |   |   |   |--- local <= 0.16\n",
            "|   |   |   |   |   |   |--- class: positive\n",
            "|   |   |   |   |   |--- local >  0.16\n",
            "|   |   |   |   |   |   |--- class: negative\n",
            "|   |   |   |   |--- carry >  0.14\n",
            "|   |   |   |   |   |--- class: negative\n",
            "|   |   |   |--- bag >  0.04\n",
            "|   |   |   |   |--- decent <= 0.08\n",
            "|   |   |   |   |   |--- class: negative\n",
            "|   |   |   |   |--- decent >  0.08\n",
            "|   |   |   |   |   |--- class: positive\n",
            "|   |--- location >  0.04\n",
            "|   |   |--- like <= 0.07\n",
            "|   |   |   |--- manager <= 0.05\n",
            "|   |   |   |   |--- coffee <= 0.20\n",
            "|   |   |   |   |   |--- class: positive\n",
            "|   |   |   |   |--- coffee >  0.20\n",
            "|   |   |   |   |   |--- class: negative\n",
            "|   |   |   |--- manager >  0.05\n",
            "|   |   |   |   |--- class: negative\n",
            "|   |   |--- like >  0.07\n",
            "|   |   |   |--- fluffy <= 0.06\n",
            "|   |   |   |   |--- class: negative\n",
            "|   |   |   |--- fluffy >  0.06\n",
            "|   |   |   |   |--- class: positive\n",
            "|--- great >  0.02\n",
            "|   |--- poor <= 0.07\n",
            "|   |   |--- disappointing <= 0.04\n",
            "|   |   |   |--- mention <= 0.04\n",
            "|   |   |   |   |--- urine <= 0.04\n",
            "|   |   |   |   |   |--- advantage <= 0.14\n",
            "|   |   |   |   |   |   |--- clothing <= 0.13\n",
            "|   |   |   |   |   |   |   |--- standing <= 0.06\n",
            "|   |   |   |   |   |   |   |   |--- wish <= 0.14\n",
            "|   |   |   |   |   |   |   |   |   |--- needless <= 0.10\n",
            "|   |   |   |   |   |   |   |   |   |   |--- build <= 0.08\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |   |   |   |   |   |   |   |--- build >  0.08\n",
            "|   |   |   |   |   |   |   |   |   |   |   |--- class: negative\n",
            "|   |   |   |   |   |   |   |   |   |--- needless >  0.10\n",
            "|   |   |   |   |   |   |   |   |   |   |--- class: negative\n",
            "|   |   |   |   |   |   |   |   |--- wish >  0.14\n",
            "|   |   |   |   |   |   |   |   |   |--- class: negative\n",
            "|   |   |   |   |   |   |   |--- standing >  0.06\n",
            "|   |   |   |   |   |   |   |   |--- class: negative\n",
            "|   |   |   |   |   |   |--- clothing >  0.13\n",
            "|   |   |   |   |   |   |   |--- class: negative\n",
            "|   |   |   |   |   |--- advantage >  0.14\n",
            "|   |   |   |   |   |   |--- class: negative\n",
            "|   |   |   |   |--- urine >  0.04\n",
            "|   |   |   |   |   |--- class: negative\n",
            "|   |   |   |--- mention >  0.04\n",
            "|   |   |   |   |--- class: negative\n",
            "|   |   |--- disappointing >  0.04\n",
            "|   |   |   |--- class: negative\n",
            "|   |--- poor >  0.07\n",
            "|   |   |--- sunday <= 0.06\n",
            "|   |   |   |--- class: negative\n",
            "|   |   |--- sunday >  0.06\n",
            "|   |   |   |--- class: positive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import export_text\n",
        "\n",
        "# Export tree structure as text\n",
        "tree_rules = export_text(decision_tree_classifier, feature_names=vectorizer.get_feature_names_out())\n",
        "print(tree_rules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zVbh4NZq6204",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Comprehensive text preprocessing function\"\"\"\n",
        "    # Remove tags\n",
        "    text = re.sub('<.*?>', '', str(text))\n",
        "\n",
        "    # Remove special characters\n",
        "    text = ''.join([x if x.isalnum() else ' ' for x in text])\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    wordnet = WordNetLemmatizer()\n",
        "    words = [wordnet.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hGCi2hu_6-_P",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def predict_text(text):\n",
        "    # Load the saved model\n",
        "    decision_tree_classifier = load_model('tree_classifier_model.pkl')\n",
        "    vectorizer = load_model('vectorizer.pkl')\n",
        "\n",
        "    # Preprocess the input text\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    # Vectorize using the same vectorizer\n",
        "    vector = vectorizer.transform([processed_text])\n",
        "\n",
        "    # Predict\n",
        "    prediction = decision_tree_classifier.predict(vector)\n",
        "\n",
        "    return prediction[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knqk9ZcjhIQt"
      },
      "source": [
        "## Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SN4rf6GvhLp4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "knn_model = load_model('knn_classifier_model.pkl')\n",
        "naive_bayes_model = load_model('bayes_classifier_model.pkl')\n",
        "decision_tree_model = load_model('tree_classifier_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDbVFg5WgtWv"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZlP7ctwxbDF",
        "outputId": "4536f0a4-5cb7-4f7b-bb45-cfb2313bfd27",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive 50\n",
            "negative 50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'positive': -64.80720209968936, 'negative': -56.770639781379366}\n",
            "KNN Predicted Label: positive\n",
            "Naive Bayes Predicted Label: negative\n",
            "Decision Trees Predicted Label: negative\n"
          ]
        }
      ],
      "source": [
        "# Test predictions\n",
        "test_texts = ['''Thien is the best ugly boi in the the gioi''']\n",
        "\n",
        "for text in test_texts:\n",
        "    knn_prediction = knn_model.predict(text)\n",
        "    nb_prediction = naive_bayes_model.predict(text)\n",
        "    dt_prediction = predict_text(text)\n",
        "    print(f\"KNN Predicted Label: {knn_prediction}\")\n",
        "    print(f\"Naive Bayes Predicted Label: {nb_prediction}\")\n",
        "    print(f\"Decision Trees Predicted Label: {dt_prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BXPMFLmy3Yg1"
      },
      "outputs": [],
      "source": [
        "!pip install flask flask_cors pyngrok\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKeDDWUF3ahN",
        "outputId": "e70746ae-509c-4856-b9c2-97e150b67bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global URL: https://6e7c-58-186-78-230.ngrok-free.app                                               \n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
            " * Running on http://127.0.0.1:8000\n",
            "Press CTRL+C to quit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive 50\n",
            "negative 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [10/Dec/2024 02:49:10] \"POST /api/v1/reviews HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS, cross_origin\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"2pzbwE4yIbzv7a2zPeAddecREOJ_yiihsHcPQFezq7RxsSLL\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "cors = CORS(app)\n",
        "app.config['CORS_HEADERS'] = 'Content-Type'\n",
        "\n",
        "@app.route('/', methods=['GET'])\n",
        "@cross_origin()\n",
        "def root():\n",
        "  return \"Hotels review\"\n",
        "# Model API\n",
        "@app.route('/api/v1/reviews', methods=['POST'])\n",
        "@cross_origin()\n",
        "def classify():\n",
        "    # Message format\n",
        "    # {\n",
        "    #     \"comments\": \"article text to classify\",\n",
        "    #     \"model\": \"knn/bayes/tree\",\n",
        "    # }\n",
        "\n",
        "    data = request.get_json()\n",
        "\n",
        "    if \"comments\" not in data:\n",
        "        return jsonify({\n",
        "            \"error\": \"Please provide a comments to classify\"\n",
        "        })\n",
        "\n",
        "    userQuery = data[\"comments\"]\n",
        "    model = data.get(\"model\", \"bayes\")\n",
        "\n",
        "    prediction = None\n",
        "    if model == \"knn\":\n",
        "        prediction = knn_model.predict(userQuery)\n",
        "    elif model == \"bayes\":\n",
        "        prediction = naive_bayes_model.predict(userQuery)\n",
        "    elif model == \"tree\":\n",
        "        prediction = predict_text(userQuery)\n",
        "    else:\n",
        "        return jsonify({\n",
        "            \"error\": \"Invalid model\"\n",
        "        })\n",
        "\n",
        "    return jsonify({\n",
        "        \"result\": prediction\n",
        "    })\n",
        "\n",
        "url = ngrok.connect(8000).public_url\n",
        "print('Global URL:', url)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=False, port=8000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6266959,
          "sourceId": 10151310,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30804,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
